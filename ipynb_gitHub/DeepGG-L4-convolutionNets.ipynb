{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepLearning / Convolutional networks\n",
    "\n",
    "## Lecture\n",
    "\n",
    "다른 위치에 있어도 같은 텍스트/이미지 즉, 같은 대상/피쳐가 있을수 있는데, 공간정보등을 인지 안하고 일일이 학습시키는 것은 비효율적\n",
    "어느 위치에 있던, 어느 컨텍스트에 있던 고양이는 고양이님. 사실상 같음. \"Statistical Invariance\"\n",
    "![L4-1]( ./images/L4-1a-motiv.png)\n",
    "![L4-1]( ./images/L4-1b-motiv.png)\n",
    "\n",
    "ConvNet은 Weight Sharing을 통해서 분별하는 접근.  \n",
    "중요한 개념으로 , feature map, pooling, patch/filter/kernel, stride 등이 있음  \n",
    "자세한건 강의참고.\n",
    "\n",
    "![Conv]( ./images/L4-2a-Conv.png)\n",
    "![Conv]( ./images/L4-2c-Conv.png)\n",
    "![Conv]( ./images/L4-2d-Conv.png)\n",
    "![Conv]( ./images/L4-2g-Conv.png)\n",
    "![Conv]( ./images/L4-2e-Conv.png)\n",
    "![Conv]( ./images/L4-2f-Conv.png)\n",
    "\n",
    "\n",
    "## Assignmnet 4 \n",
    "\n",
    "Previously in 2_fullyconnected.ipynb and 3_regularization.ipynb, we trained fully connected networks to classify notMNIST characters.\n",
    "The goal of this assignment is make the neural network convolutional.\n",
    "\n",
    "\n",
    "** References:** \n",
    "- [Arn-O](https://github.com/Arn-O/udacity-deep-learning/blob/master/4_convolutions.ipynb)  \n",
    "\n",
    "** Other ConvNet examples:**\n",
    "- [aymericdamien](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/convolutional_network.ipynb)\n",
    "- [terryum](https://github.com/terryum/TensorFlow_Exercises/blob/master/4a_CNN_MNIST_160517.ipynb)\n",
    "- [tensorflow tutorial/ codeOnWeb](https://codeonweb.com/entry/f50e23df-0f23-4e56-95a6-efb9981716f7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save\n",
    "    \n",
    "    print ('Training Set', train_dataset.shape, train_labels.shape)\n",
    "    print ('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print ('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "리포맷  \n",
    "Reformat into a TensorFlow-friendly shape: \n",
    "- convolutions need the image dadta formatted as a cube(width X height X # of channels )\n",
    "- labels as float 1-hot encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set (200000, 28, 28, 1) (200000, 10)\n",
      "Validation Set (10000, 28, 28, 1) (200000, 10)\n",
      "Test set (10000, 28, 28, 1) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size =  28\n",
    "num_labels = 10\n",
    "num_channels = 1 #gray scale\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "    labels = (np.arange(num_labels) == labels[:, None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "\n",
    "print ('Training Set', train_dataset.shape, train_labels.shape)\n",
    "print ('Validation Set', valid_dataset.shape, train_labels.shape)\n",
    "print ('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return( 100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels,1))/predictions.shape[0]  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "콘볼루션 레이어(2 layer)와 fully connected layer 구현:\n",
    "\n",
    "Let's build a small network with two convolutional layers, followed by one fully connected layer.  \n",
    "Convolutional networks are more expensive computationally,  \n",
    "so we'll limit its depth and number of fully connected nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    #input data\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape = (batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape = (batch_size, num_labels) )\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    #Variables \n",
    "    layer1_weights = tf.Variable(tf.truncated_normal( [patch_size, patch_size, num_channels, depth], stddev = 0.1) )\n",
    "    layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "    \n",
    "    layer2_weights = tf.Variable(tf.truncated_normal( [patch_size, patch_size, depth, depth], stddev=0.1) )\n",
    "    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "    \n",
    "    layer3_weights = tf.Variable(tf.truncated_normal( [image_size/4*image_size/4*depth, num_hidden], stddev = 0.1) )\n",
    "    layer3_biases = tf.Variable(tf.constant(1.0,shape = [num_hidden] ))\n",
    "    \n",
    "    layer4_weights = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev=0.1 ))\n",
    "    layer4_biases = tf.Variable(tf.constant(1.0, shape = [num_labels]))\n",
    "    \n",
    "    # Model\n",
    "    def model(data):\n",
    "        conv = tf.nn.conv2d(data, layer1_weights, [1,2,2,1], padding = 'SAME')\n",
    "        hidden = tf.nn.relu(conv + layer1_biases)\n",
    "        conv = tf.nn.conv2d(hidden, layer2_weights,[1,2,2,1], padding = 'SAME' )\n",
    "        hidden = tf.nn.relu(conv + layer2_biases)\n",
    "\n",
    "        shape = hidden.get_shape().as_list()\n",
    "        reshape = tf.reshape(hidden, [shape[0], shape[1]*shape[2]*shape[3] ])\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "        \n",
    "        return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "    \n",
    "\n",
    "    # Training Computation:\n",
    "    logits = model(tf_train_dataset)\n",
    "    loss= tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(.05).minimize(loss)\n",
    "    \n",
    "    # Predictions for the training, validation, and test data\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** :Execute **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step: 0: 3.171521\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 10.0%\n",
      "Minibatch loss at step: 50: 1.578632\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 47.5%\n",
      "Minibatch loss at step: 100: 1.237140\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 70.1%\n",
      "Minibatch loss at step: 150: 0.672105\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 75.6%\n",
      "Minibatch loss at step: 200: 0.801610\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 77.5%\n",
      "Minibatch loss at step: 250: 1.090486\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 79.0%\n",
      "Minibatch loss at step: 300: 0.544587\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 79.7%\n",
      "Minibatch loss at step: 350: 0.766908\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 78.0%\n",
      "Minibatch loss at step: 400: 0.373723\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 78.0%\n",
      "Minibatch loss at step: 450: 0.735488\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 79.0%\n",
      "Minibatch loss at step: 500: 0.786490\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 80.9%\n",
      "Minibatch loss at step: 550: 0.843940\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step: 600: 0.173876\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 81.1%\n",
      "Minibatch loss at step: 650: 0.892876\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 80.8%\n",
      "Minibatch loss at step: 700: 0.830600\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 82.3%\n",
      "Minibatch loss at step: 750: 0.156883\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 82.3%\n",
      "Minibatch loss at step: 800: 0.887911\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step: 850: 0.759340\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step: 900: 0.617068\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step: 950: 0.627233\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.1%\n",
      "Minibatch loss at step: 1000: 0.538335\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.9%\n",
      "Test Accuracy: 89.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph = graph ) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print ('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset+ batch_size), :]\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n",
    "        _,l,predictions = session.run([optimizer, loss, train_prediction], feed_dict= feed_dict)\n",
    "        if (step%50 ==0):\n",
    "            print ('Minibatch loss at step: %d: %f' % (step, l))\n",
    "            print ('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            print ('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n",
    "\n",
    "    print('Test Accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Problem 1\n",
    "\n",
    "The convolutional model above uses convolutions with stride 2 to reduce the dimensionality. Replace the strides by a max pooling operation(i.e. nn.max_pool()) of stride 2 and kernel size2.\n",
    "\n",
    "** * 요약: **\n",
    "위 과정은 stride 값이 2인 covolution 을 통해 차원 축소를 하였음.  \n",
    "이를 maxpooling , kernel size, stride=2 의 조건으로 재 시행.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Input Data\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape = (batch_size, image_size, image_size, num_channels) )\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape = (batch_size, num_labels) )\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    # Variable\n",
    "    layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth], stddev = 0.1))\n",
    "    layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "    \n",
    "    layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, depth], stddev = 0.1))\n",
    "    layer2_biases = tf.Variable(tf.constant(1.0, shape = [depth]))\n",
    "    \n",
    "    layer3_weights = tf.Variable(tf.truncated_normal([image_size/4 * image_size/4 * depth, num_hidden], stddev=0.1))\n",
    "    layer3_biases = tf.Variable(tf.constant(1.0, shape = [num_hidden]))\n",
    "    \n",
    "    layer4_weights = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev=0.1))\n",
    "    layer4_biases = tf.Variable(tf.constant(1.0, shape = [num_labels])) \n",
    "    \n",
    "    def model(data):\n",
    "        conv1 = tf.nn.conv2d(data, layer1_weights, [1,1,1,1], padding = 'SAME')\n",
    "        bias1 = tf.nn.relu(conv1 + layer1_biases)\n",
    "        pool1 = tf.nn.max_pool(bias1, [1,2,2,1], [1,2,2,1], padding = 'SAME')\n",
    "\n",
    "        conv2 = tf.nn.conv2d(pool1, layer2_weights, [1,1,1,1], padding = 'SAME')\n",
    "        bias2 = tf.nn.relu(conv2 + layer2_biases)\n",
    "        pool2 = tf.nn.max_pool(bias2, [1,2,2,1], [1,2,2,1], padding = 'SAME')\n",
    "\n",
    "        shape = pool2.get_shape().as_list()\n",
    "        reshape = tf.reshape(pool2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "\n",
    "        return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "    \n",
    "    # Training Computation\n",
    "    logits = model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(.05).minimize(loss)\n",
    "    \n",
    "    # Prediction\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Running the Session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.173182\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 11.7%\n",
      "Minibatch loss at step 50: 2.246892\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 25.6%\n",
      "Minibatch loss at step 100: 1.581455\n",
      "Minibatch accuracy: 37.5%\n",
      "Validation accuracy: 53.0%\n",
      "Minibatch loss at step 150: 0.845521\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 70.5%\n",
      "Minibatch loss at step 200: 1.000629\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 75.6%\n",
      "Minibatch loss at step 250: 1.362105\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 77.7%\n",
      "Minibatch loss at step 300: 0.510090\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 80.3%\n",
      "Minibatch loss at step 350: 0.842779\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 78.0%\n",
      "Minibatch loss at step 400: 0.255478\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 80.3%\n",
      "Minibatch loss at step 450: 0.617679\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 78.8%\n",
      "Minibatch loss at step 500: 0.815000\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 80.0%\n",
      "Minibatch loss at step 550: 0.797583\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.0%\n",
      "Minibatch loss at step 600: 0.159749\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 650: 1.140612\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.8%\n",
      "Minibatch loss at step 700: 0.671989\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.4%\n",
      "Minibatch loss at step 750: 0.054205\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 82.6%\n",
      "Minibatch loss at step 800: 0.826924\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 82.1%\n",
      "Minibatch loss at step 850: 0.733985\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.3%\n",
      "Minibatch loss at step 900: 0.587719\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.9%\n",
      "Minibatch loss at step 950: 0.731324\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 84.0%\n",
      "Minibatch loss at step 1000: 0.537463\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 84.4%\n",
      "Test Accuracy: 90.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph = graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        \n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels: batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict= feed_dict)\n",
    "        \n",
    "        if(step%50 == 0):\n",
    "            print ('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print ('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            print ('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels ))\n",
    "        \n",
    "    print('Test Accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Problem 2\n",
    "Try to get the best performance you can using a convolutional net. Look for example at the classic [LeNet5](http://yann.lecun.com/exdb/lenet/) architecture,  \n",
    "adding Dropout, and/or adding learning rate decay.\n",
    "\n",
    "** *요약: Dropout, learning rate deacy를 써보자. **\n",
    "\n",
    "** - NO Dropout &  Learning Rate Decay  Version- **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "    #Input Data\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape = [batch_size, image_size, image_size, num_channels] )\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape = [batch_size, num_labels] )\n",
    "\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    #Variables \n",
    "    layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth], stddev=.1))\n",
    "    layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "    \n",
    "    layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, depth], stddev = .1 ))\n",
    "    layer2_biases = tf.Variable(tf.constant(1.0 , shape = [depth]))\n",
    "\n",
    "    size3 = ( (image_size - patch_size + 1 )/2 - patch_size + 1 )/2    \n",
    "    \n",
    "    layer3_weights = tf.Variable(tf.truncated_normal([size3 * size3 * depth, num_hidden], stddev= .1))\n",
    "    layer3_biases = tf.Variable(tf.constant(1.0, shape = [num_hidden]))\n",
    "    \n",
    "    layer4_weights = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev=0.1))\n",
    "    layer4_biases = tf.Variable(tf.constant(1.0, shape = [num_labels]))\n",
    "    \n",
    "    def model(data):\n",
    "        # C1 input 28 X 28\n",
    "        conv1 = tf.nn.conv2d(data, layer1_weights, [1,1,1,1], padding = 'VALID')\n",
    "        bias1 = tf.nn.relu(conv1+ layer1_biases)\n",
    "        \n",
    "        # S2 input 24 X 24\n",
    "        pool2 = tf.nn.avg_pool(bias1, [1,2,2,1], [1,2,2,1], padding = 'VALID')\n",
    "        \n",
    "        # C3 input 12 X 12\n",
    "        conv3 = tf.nn.conv2d(pool2, layer2_weights, [1,1,1,1], padding = 'VALID')\n",
    "        bias3 = tf.nn.relu(conv3+ layer2_biases)\n",
    "        \n",
    "        # S4 input 8 X 8 \n",
    "        pool4 = tf.nn.avg_pool(bias3, [1,2,2,1], [1,2,2,1], padding = 'VALID')\n",
    "        \n",
    "        # F6 input 4 X 4\n",
    "        shape = pool4.get_shape().as_list()\n",
    "        reshape = tf.reshape(pool4, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases) \n",
    "        \n",
    "        return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "    \n",
    "    # Training computation\n",
    "    logits = model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(.05).minimize(loss)\n",
    "    \n",
    "    # Predictions for the training, validation, and test data\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the Session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.483344\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 10.0%\n",
      "Minibatch loss at step 100: 1.319547\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 60.1%\n",
      "Minibatch loss at step 200: 0.844818\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 72.5%\n",
      "Minibatch loss at step 300: 0.658113\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 75.2%\n",
      "Minibatch loss at step 400: 0.396682\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 77.3%\n",
      "Minibatch loss at step 500: 0.763901\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 78.7%\n",
      "Minibatch loss at step 600: 0.263028\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 79.8%\n",
      "Minibatch loss at step 700: 0.962759\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 80.4%\n",
      "Minibatch loss at step 800: 0.605703\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 900: 0.568709\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.1%\n",
      "Minibatch loss at step 1000: 0.623930\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.9%\n",
      "Minibatch loss at step 1100: 0.652747\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.0%\n",
      "Minibatch loss at step 1200: 0.812228\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 1300: 0.331175\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 82.3%\n",
      "Minibatch loss at step 1400: 0.264914\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 82.9%\n",
      "Minibatch loss at step 1500: 0.671316\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 1600: 1.088815\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 1700: 1.008641\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 83.9%\n",
      "Minibatch loss at step 1800: 0.583934\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.5%\n",
      "Minibatch loss at step 1900: 0.570454\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.8%\n",
      "Minibatch loss at step 2000: 0.079884\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 84.2%\n",
      "Test Accuracy: 90.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 2001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        \n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels : batch_labels}\n",
    "        _,l,predictions = session.run([optimizer, loss, train_prediction], feed_dict = feed_dict)\n",
    "        \n",
    "        if(step%100 == 0):\n",
    "            print ('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print ('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            print ('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels ))\n",
    "        \n",
    "    print('Test Accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** - Dropout &  Learning Rate Decay  Version- **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "\n",
    "num_hidden = 64\n",
    "beta_regul = 1e-3\n",
    "drop_out = .5\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "    #Input data\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape = (batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels= tf.placeholder(tf.float32, shape=(batch_size, num_labels) )\n",
    "\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    global_step = tf.Variable(0)\n",
    "    \n",
    "    #Variables\n",
    "    layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth], stddev= 0.1) )\n",
    "    layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "    \n",
    "    layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "    layer2_biases = tf.Variable(tf.constant(1.0, shape = [depth]))\n",
    "    \n",
    "    size3= ((image_size-patch_size +1)/2 - patch_size +1)/2 \n",
    "    \n",
    "    layer3_weights = tf.Variable(tf.truncated_normal([size3*size3*depth, num_hidden], stddev=-.1))\n",
    "    layer3_biases = tf.Variable(tf.constant(1.0, shape = [num_hidden]))\n",
    "    \n",
    "    layer4_weights = tf.Variable(tf.truncated_normal([num_hidden, num_hidden], stddev =0.1))\n",
    "    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "    \n",
    "    layer5_weights = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev = 0.1))\n",
    "    layer5_biases  = tf.Variable(tf.constant(1.0, shape = [num_labels]))\n",
    "    \n",
    "    # Model\n",
    "    def model(data, keep_prob):\n",
    "        # C1 input 28 X 28\n",
    "        conv1 = tf.nn.conv2d(data, layer1_weights, [1,1,1,1], padding = 'VALID')\n",
    "        bias1 = tf.nn.relu(conv1+layer1_biases)\n",
    "        \n",
    "        # S2 input 24 X 24\n",
    "        pool2 = tf.nn.avg_pool(bias1, [1,2,2,1], [1,2,2,1], padding = 'VALID')\n",
    "        \n",
    "        # C3 input 12 X 12\n",
    "        conv3 = tf.nn.conv2d(pool2, layer2_weights, [1,1,1,1], padding = 'VALID')\n",
    "        bias3 = tf.nn.relu(conv3 + layer2_biases)\n",
    "        \n",
    "        # S4 input 8 X 8 \n",
    "        pool4 = tf.nn.avg_pool(bias3, [1,2,2,1],[1,2,2,1], padding = 'VALID')\n",
    "        \n",
    "        # F5 input 4 X 4\n",
    "        shape = pool4.get_shape().as_list()\n",
    "        reshape = tf.reshape(pool4, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden5 = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "        \n",
    "        # F6 \n",
    "        drop5 = tf.nn.dropout(hidden5, keep_prob)\n",
    "        hidden6 = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "        drop6 = tf.nn.dropout(hidden6, keep_prob)\n",
    "        return tf.matmul(drop6, layer5_weights) + layer5_biases\n",
    "    \n",
    "    # Training computation\n",
    "    logits = model(tf_train_dataset, drop_out)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "    # Optimizer\n",
    "    learning_rate = tf.train.exponential_decay(0.05, global_step, 1000, 0.85, staircase = True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step= global_step)\n",
    "    \n",
    "    # predictions for the training, validation, and test data\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset, 1.0))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.309491\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 12.3%\n",
      "Minibatch loss at step 500: 1.146324\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 75.2%\n",
      "Minibatch loss at step 1000: 0.657059\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 1500: 0.888884\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 2000: 0.178765\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 2500: 0.587984\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 3000: 0.985930\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.7%\n",
      "Minibatch loss at step 3500: 0.880156\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.7%\n",
      "Minibatch loss at step 4000: 0.548153\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 84.7%\n",
      "Minibatch loss at step 4500: 0.874191\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 85.1%\n",
      "Minibatch loss at step 5000: 1.302955\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 85.1%\n",
      "Test accuracy: 91.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 5001\n",
    "\n",
    "with tf.Session(graph = graph ) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        offset = (step*batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset: (offset+batch_size), :,:,:]\n",
    "        batch_labels = train_labels[offset: (offset+batch_size), :]\n",
    "\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict = feed_dict)\n",
    "        \n",
    "        if(step %500 ==0):\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
